{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBV Classification\n",
    "\n",
    "In this notebook I train and validate the 3D CNN proposed by Mendoza for Next Best View planning.\n",
    "\n",
    "J. Irving Vasquez-Gomez\n",
    "\n",
    "jivg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "display_dataset = True\n",
    "display_fwd_pretraining = True\n",
    "load_weights = False\n",
    "reading_weights_file = 'weights/nbv_net_no_drop.pth'\n",
    "saving_weights_file = 'weights/nbv_net_drop_0_epoch_1000_lr_001_batch_1000.pth'\n",
    "epochs = 1000\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import classification_nbv as cnbv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pose that corresponds to a class.\n",
    "nbv_positions = np.genfromtxt('points_in_sphere.txt')\n",
    "\n",
    "# This function converts a class to its corresponding pose\n",
    "def getPosition(nbv_class, nbv_positions):\n",
    "    return nbv_positions[nbv_class]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = cnbv.NBVClassificationDatasetFull(grid_file='../dataset/classification/training/dataset_vol_classification_training.npy', \n",
    "                                    nbv_class_file='../dataset/classification/training/dataset_lbl_classification_training.npy',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    # Reshapes the plain grid\n",
    "                                    cnbv.To3DGrid(),\n",
    "                                    #converts to tensors\n",
    "                                    cnbv.ToTensor()\n",
    "                                    ]))\n",
    "\n",
    "validation_dataset = cnbv.NBVClassificationDatasetFull(grid_file='../dataset/classification/validation/dataset_vol_classification_validation.npy', \n",
    "                                    nbv_class_file='../dataset/classification/validation/dataset_lbl_classification_validation.npy',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    # Reshapes the plain grid\n",
    "                                    cnbv.To3DGrid(),\n",
    "                                    #converts to tensors\n",
    "                                    cnbv.ToTensor()\n",
    "                                    ]))\n",
    "\n",
    "print('Training dataset lenght:' + str(len(training_dataset)))\n",
    "print('Validation dataset lenght:' + str(len(validation_dataset)))\n",
    "\n",
    "for i in range(len(training_dataset)):\n",
    "    sample = training_dataset[i]\n",
    "\n",
    "    print(i, sample['grid'].size(), sample['nbv_class'].size())\n",
    "    \n",
    "    if display_dataset:\n",
    "        #print(sample['nbv_class'].numpy())\n",
    "        nbv = getPosition(sample['nbv_class'].numpy(), nbv_positions)\n",
    "        nbv = np.squeeze(nbv)\n",
    "        #print(nbv)\n",
    "        cnbv.showGrid(sample['grid'].numpy(), nbv)\n",
    "    \n",
    "    if i == 3:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data in batches\n",
    "#batch_size = 500\n",
    "\n",
    "train_loader = DataLoader(training_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBV-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBV_net proposed in Mendoza's Master thesis\n",
    "\n",
    "class NBV_Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(NBV_Net, self).__init__()\n",
    "        \n",
    "        dropout_prob = 0.0 # 1 - 0.7\n",
    "\n",
    "        # Three 3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(1, 10, 3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(2,2,2), stride = (2,2,2))       \n",
    "\n",
    "        self.conv2 = nn.Conv3d(10, 12, 3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2,2,2), stride = (2,2,2))\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(12, 8, 3, stride=1, padding=1)\n",
    "        self.conv3_drop = nn.Dropout(dropout_prob)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2,2,2), stride = (2,2,2))      \n",
    "\n",
    "        # Five fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 1500)   \n",
    "        self.fc1_drop = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(1500, 500)\n",
    "        self.fc2_drop = nn.Dropout(dropout_prob)      \n",
    "\n",
    "        self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc3_drop = nn.Dropout(dropout_prob)      \n",
    "\n",
    "        self.fc4 = nn.Linear(100, 50)\n",
    "        self.fc4_drop = nn.Dropout(dropout_prob)   \n",
    "        \n",
    "        self.fc5 = nn.Linear(50, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## feedforward behavior of NBV-net\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Aplanar\n",
    "        x = x.view(x.size(0), -1)\n",
    "               \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)      \n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)       \n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc3_drop(x)       \n",
    "\n",
    "        x = F.relu(self.fc4(x)) \n",
    "        x = self.fc4_drop(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x)) \n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NBV_Net()\n",
    "net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "net.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights load\n",
    "if load_weights:\n",
    "    state_dict = torch.load(reading_weights_file)\n",
    "    #print(state_dict.keys())\n",
    "    net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on a batch of test grids\n",
    "def net_sample_output():\n",
    "    net.eval()\n",
    "    \n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        grids = sample['grid']\n",
    "        nbvs = sample['nbv_class']\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "        grids = grids.type(torch.FloatTensor)\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        grids = Variable(grids)    \n",
    "        grids = grids.to(device)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output = net(grids)\n",
    "        grids = grids.cpu()\n",
    "        output = output.cpu()\n",
    "        \n",
    "        # get the predicted class from the maximum value in the output-list of class scores\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        # break after first image is tested\n",
    "        if i == 0:\n",
    "            return grids, predicted, nbvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function that test the model on a batch\n",
    "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
    "test_grids, test_outputs, gt_nbvs = net_sample_output()\n",
    "\n",
    "# print out the dimensions of the data to see if they make sense\n",
    "print(test_grids.data.size())\n",
    "print(test_outputs.data.size())\n",
    "#print(test_outputs)\n",
    "print(gt_nbvs.size())\n",
    "#print(gt_nbvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful function that displays the octree and the nbv\n",
    "def visualize_output(test_grids, test_outputs, gt_nbvs=None, batch_size=batch_size):\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        #plt.figure(figsize=(20,10))\n",
    "        #ax = plt.subplot(1, batch_size, i+1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        grid = test_grids[i].data   # get the image from it's wrapper\n",
    "        grid = grid.numpy()   # convert to numpy array from a Tensor\n",
    "        #image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "        print(test_outputs[i].data.numpy())\n",
    "        # un-transform the predicted nbv\n",
    "        #angles = test_outputs[i].data.numpy()\n",
    "        #angles = lnbv.normPos2Angles(angles)\n",
    "        #yaw, pitch = angles\n",
    "        #r = 0.4      \n",
    "        predicted = getPosition( test_outputs[i].data.numpy(), nbv_positions)\n",
    "        #print(predicted)\n",
    "        \n",
    "        print(gt_nbvs[i].numpy())\n",
    "        #gt_angles = gt_nbvs[i].numpy()\n",
    "        #gt_angles = lnbv.normPos2Angles(gt_angles)\n",
    "        #gt_yaw, gt_pitch = gt_angles\n",
    "        #print(np.array([r, gt_yaw, gt_pitch]))\n",
    "        gt = getPosition(gt_nbvs[i].numpy(), nbv_positions)\n",
    "        #print(gt)\n",
    "        \n",
    "        cnbv.showGrid(grid, gt, predicted)\n",
    "\n",
    "        #plt.show()\n",
    "        \n",
    "        if i==2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_output(test_grids, test_outputs, np.squeeze(gt_nbvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Function\n",
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for sample in testloader:\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        grids = sample['grid']\n",
    "        nbvs = sample['nbv_class']\n",
    "        \n",
    "        # convert images to FloatTensors\n",
    "        grids = grids.type(torch.FloatTensor)\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        grids = Variable(grids)    \n",
    "        grids = grids.to(device)\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        nbvs = Variable(nbvs) \n",
    "        nbvs = nbvs.to(device)\n",
    "\n",
    "        output = model.forward(grids)\n",
    "        test_loss += criterion(output, nbvs).item()\n",
    "\n",
    "        # for log.  ps = torch.exp(output)\n",
    "        equality = (nbvs.data == output.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "#epochs = 10\n",
    "running_loss = 0\n",
    "save_after = 100\n",
    "\n",
    "history_epoch = []\n",
    "history_train_loss = []\n",
    "history_validation_loss = []\n",
    "history_train_accuracy = []\n",
    "history_validation_accuracy = []\n",
    "\n",
    "import time\n",
    "tic = time.clock()\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Cambiamos a modo entrenamiento\n",
    "    net.train()\n",
    "    \n",
    "    for i, sample in enumerate(train_loader):        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        grids = sample['grid']\n",
    "        nbvs = sample['nbv_class']\n",
    "\n",
    "        # convert grids to FloatTensors\n",
    "        grids = grids.type(torch.FloatTensor)\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        grids = Variable(grids)    \n",
    "        grids = grids.to(device)\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        nbvs = Variable(nbvs) \n",
    "        nbvs = nbvs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output = net(grids)\n",
    "        \n",
    "        #ot = output.cpu()\n",
    "        #print(ot)    \n",
    "        \n",
    "        loss = criterion(output, nbvs)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Optimización\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    # Cambiamos a modo de evaluación\n",
    "    net.eval()\n",
    "            \n",
    "    # Apagamos los gradientes, reduce memoria y cálculos\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_accuracy = validation(net, train_loader, criterion)\n",
    "        val_loss, val_accuracy = validation(net, validation_loader, criterion)\n",
    "        \n",
    "        train_loss, train_accuracy = train_loss, train_accuracy.cpu().numpy()\n",
    "        val_loss, val_accuracy = val_loss, val_accuracy.cpu().numpy()\n",
    "        \n",
    "        train_accuracy = train_accuracy / len(train_loader)\n",
    "        val_accuracy = val_accuracy / len(validation_loader)\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Val. Loss: {:.3f}.. \".format(val_loss),\n",
    "              \"Train Accuracy: {:.3f}\".format(train_accuracy),\n",
    "              \"Val. Accuracy: {:.3f}\".format(val_accuracy))\n",
    "    \n",
    "    history_epoch.append(e)\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_validation_loss.append(val_loss)\n",
    "    history_train_accuracy.append(train_accuracy)\n",
    "    history_validation_accuracy.append(val_accuracy)\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    if(e % save_after == 0):\n",
    "        np.save('log/train_loss', history_train_loss)\n",
    "        np.save('log/validation_loss', history_validation_loss)\n",
    "        np.save('log/train_accuracy', history_train_accuracy)\n",
    "        np.save('log/validation_accuracy', history_validation_accuracy)\n",
    "        torch.save(net.state_dict(), 'log/weights.pth')\n",
    "    \n",
    "    # Make sure training is back on\n",
    "    net.train()\n",
    "    \n",
    "toc = time.clock()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_epoch, history_train_loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_epoch, history_train_accuracy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train set accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_epoch, history_validation_accuracy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation set accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
    "test_grids, test_outputs, gt_nbvs = net_sample_output()\n",
    "visualize_output(test_grids, test_outputs, np.squeeze(gt_nbvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters\n",
    "torch.save(net.state_dict(), saving_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics\n",
    "\n",
    "np.save('log/train_loss', history_train_loss)\n",
    "np.save('log/validation_loss', history_validation_loss)\n",
    "np.save('log/train_accuracy', history_train_accuracy)\n",
    "np.save('log/validation_accuracy', history_validation_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
